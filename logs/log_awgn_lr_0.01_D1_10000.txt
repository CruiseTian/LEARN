Namespace(D=1, batch_size=500, block_len=100, block_len_high=200, block_len_low=10, channel='awgn', code_rate_k=1, code_rate_n=3, dec_act='linear', dec_lr=0.01, dec_num_layer=5, dec_num_unit=100, dec_rnn='gru', dropout=0.0, enc_act='elu', enc_clipping='both', enc_grad_limit=0.01, enc_lr=0.01, enc_num_layer=2, enc_num_unit=25, enc_quantize_level=2, enc_rnn='gru', enc_truncate_limit=0, enc_value_limit=1.0, init_nw_weight='default', is_variable_block_len=False, no_code_norm=False, no_cuda=False, num_block=10000, num_epoch=60, num_train_dec=5, num_train_enc=1, optimizer='adam', precompute_norm_stats=False, print_pos_ber=False, print_pos_power=False, print_test_traj=False, radar_power=5.0, radar_prob=0.05, rec_quantize=False, snr_points=12, snr_test_end=4.0, snr_test_start=-1.5, test_ratio=1, train_channel_mode='block_norm', train_dec_channel_high=2.0, train_dec_channel_low=-1.5, train_enc_channel_high=1.0, train_enc_channel_low=1.0, vv=5)
use_cuda:  True
Channel_AE(
  (enc): ENC(
    (enc_rnn): GRU(1, 25, num_layers=2, batch_first=True)
    (enc_linear): Linear(in_features=25, out_features=3, bias=True)
  )
  (dec): DEC(
    (dropout): Dropout(p=0.0, inplace=False)
    (dec1_rnns): GRU(3, 100, num_layers=2, batch_first=True)
    (dec2_rnns): GRU(3, 100, num_layers=2, batch_first=True)
    (dec_outputs): Linear(in_features=200, out_features=1, bias=True)
  )
)
====> Epoch: 1 Average loss: 0.68736633  running time 3.5875227451324463
====> Epoch: 1 Average loss: 0.28557607  running time 3.5666399002075195
====> Epoch: 1 Average loss: 0.16759029  running time 3.564518928527832
====> Epoch: 1 Average loss: 0.15942739  running time 3.561192035675049
====> Epoch: 1 Average loss: 0.15749599  running time 3.577394485473633
====> Epoch: 1 Average loss: 0.15632180  running time 3.560178279876709
====> Test set BCE loss 0.1141243726015091 Custom Loss 0.1141243726015091 with ber  0.042666997760534286 with bler  0.9835
saved model ./tmp/model_1_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.94595217704773s
====> Epoch: 2 Average loss: 0.11105497  running time 3.559870719909668
====> Epoch: 2 Average loss: 0.13150280  running time 3.5861878395080566
====> Epoch: 2 Average loss: 0.12959692  running time 3.5564217567443848
====> Epoch: 2 Average loss: 0.12634015  running time 3.558018684387207
====> Epoch: 2 Average loss: 0.12649593  running time 3.5485446453094482
====> Epoch: 2 Average loss: 0.12541970  running time 3.5544395446777344
====> Test set BCE loss 0.08414444327354431 Custom Loss 0.08414444327354431 with ber  0.03120199777185917 with bler  0.9482000000000002
saved model ./tmp/model_2_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.86873984336853s
====> Epoch: 3 Average loss: 0.08178893  running time 3.529402256011963
====> Epoch: 3 Average loss: 0.11807160  running time 3.549309015274048
====> Epoch: 3 Average loss: 0.11823952  running time 3.547337532043457
====> Epoch: 3 Average loss: 0.11810310  running time 3.5638673305511475
====> Epoch: 3 Average loss: 0.11721085  running time 3.555657386779785
====> Epoch: 3 Average loss: 0.11741509  running time 3.5457301139831543
====> Test set BCE loss 0.07926023006439209 Custom Loss 0.07926023006439209 with ber  0.02884499542415142 with bler  0.9411000000000002
saved model ./tmp/model_3_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.8062961101532s
====> Epoch: 4 Average loss: 0.07739868  running time 3.5464439392089844
====> Epoch: 4 Average loss: 0.11283579  running time 3.545677900314331
====> Epoch: 4 Average loss: 0.11301493  running time 3.5434420108795166
====> Epoch: 4 Average loss: 0.11258051  running time 3.567152976989746
====> Epoch: 4 Average loss: 0.11298376  running time 3.561494827270508
====> Epoch: 4 Average loss: 0.11243477  running time 3.5569345951080322
====> Test set BCE loss 0.07454622536897659 Custom Loss 0.07454622536897659 with ber  0.027199000120162964 with bler  0.9312999999999999
saved model ./tmp/model_4_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.85402274131775s
====> Epoch: 5 Average loss: 0.07384803  running time 3.550088882446289
====> Epoch: 5 Average loss: 0.11081924  running time 3.561067819595337
====> Epoch: 5 Average loss: 0.10920717  running time 3.5677874088287354
====> Epoch: 5 Average loss: 0.10971335  running time 3.5685737133026123
====> Epoch: 5 Average loss: 0.11014671  running time 3.574856758117676
====> Epoch: 5 Average loss: 0.11060173  running time 3.5737199783325195
====> Test set BCE loss 0.07205680012702942 Custom Loss 0.07205680012702942 with ber  0.026158003136515617 with bler  0.9290000000000003
saved model ./tmp/model_5_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.93215012550354s
====> Epoch: 6 Average loss: 0.07140482  running time 3.5668656826019287
====> Epoch: 6 Average loss: 0.10636918  running time 3.5722525119781494
====> Epoch: 6 Average loss: 0.10586173  running time 3.5791749954223633
====> Epoch: 6 Average loss: 0.10576876  running time 3.5688135623931885
====> Epoch: 6 Average loss: 0.10546388  running time 3.59690523147583
====> Epoch: 6 Average loss: 0.10552851  running time 3.5769424438476562
====> Test set BCE loss 0.06844595819711685 Custom Loss 0.06844595819711685 with ber  0.024946000427007675 with bler  0.9171000000000002
saved model ./tmp/model_6_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.003576040267944s
====> Epoch: 7 Average loss: 0.06665506  running time 3.557284116744995
====> Epoch: 7 Average loss: 0.09952393  running time 3.5816612243652344
====> Epoch: 7 Average loss: 0.09872738  running time 3.5829403400421143
====> Epoch: 7 Average loss: 0.09781792  running time 3.5723941326141357
====> Epoch: 7 Average loss: 0.09775698  running time 3.5701260566711426
====> Epoch: 7 Average loss: 0.09812499  running time 3.57383131980896
====> Test set BCE loss 0.0626944974064827 Custom Loss 0.0626944974064827 with ber  0.02240699902176857 with bler  0.8934000000000001
saved model ./tmp/model_7_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.964643955230713s
====> Epoch: 8 Average loss: 0.06009263  running time 3.5634336471557617
====> Epoch: 8 Average loss: 0.09183306  running time 3.5753610134124756
====> Epoch: 8 Average loss: 0.09011838  running time 3.570164442062378
====> Epoch: 8 Average loss: 0.08954660  running time 3.5689730644226074
====> Epoch: 8 Average loss: 0.08989033  running time 3.5632240772247314
====> Epoch: 8 Average loss: 0.09048654  running time 3.567281484603882
====> Test set BCE loss 0.05457437038421631 Custom Loss 0.05457437038421631 with ber  0.01954999752342701 with bler  0.8443999999999999
saved model ./tmp/model_8_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.928858518600464s
====> Epoch: 9 Average loss: 0.05274247  running time 3.5626509189605713
====> Epoch: 9 Average loss: 0.08686496  running time 3.5966482162475586
====> Epoch: 9 Average loss: 0.08619653  running time 3.5767664909362793
====> Epoch: 9 Average loss: 0.08727998  running time 3.577176094055176
====> Epoch: 9 Average loss: 0.08631650  running time 3.58914852142334
====> Epoch: 9 Average loss: 0.08579427  running time 3.573859453201294
====> Test set BCE loss 0.050846368074417114 Custom Loss 0.050846368074417114 with ber  0.01791900210082531 with bler  0.8119999999999999
saved model ./tmp/model_9_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.001558780670166s
====> Epoch: 10 Average loss: 0.05004177  running time 3.564359426498413
====> Epoch: 10 Average loss: 0.08572382  running time 3.572416067123413
====> Epoch: 10 Average loss: 0.08500167  running time 3.5785064697265625
====> Epoch: 10 Average loss: 0.08469549  running time 3.5693633556365967
====> Epoch: 10 Average loss: 0.08481627  running time 3.572080612182617
====> Epoch: 10 Average loss: 0.08567054  running time 3.5807673931121826
====> Test set BCE loss 0.049539756029844284 Custom Loss 0.049539756029844284 with ber  0.017708998173475266 with bler  0.7998999999999999
saved model ./tmp/model_10_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.963059663772583s
====> Epoch: 11 Average loss: 0.04897129  running time 3.562572956085205
====> Epoch: 11 Average loss: 0.08528202  running time 3.582385778427124
====> Epoch: 11 Average loss: 0.08499337  running time 3.5756537914276123
====> Epoch: 11 Average loss: 0.08507689  running time 3.580890417098999
====> Epoch: 11 Average loss: 0.08574467  running time 3.5696892738342285
====> Epoch: 11 Average loss: 0.08498075  running time 3.563795328140259
====> Test set BCE loss 0.04932239279150963 Custom Loss 0.04932239279150963 with ber  0.01727999746799469 with bler  0.7842
saved model ./tmp/model_11_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.962604522705078s
====> Epoch: 12 Average loss: 0.04923736  running time 3.5640735626220703
====> Epoch: 12 Average loss: 0.08496459  running time 3.585862874984741
====> Epoch: 12 Average loss: 0.08431863  running time 3.5768637657165527
====> Epoch: 12 Average loss: 0.08525616  running time 3.58701491355896
====> Epoch: 12 Average loss: 0.08427540  running time 3.581577777862549
====> Epoch: 12 Average loss: 0.08445371  running time 3.5790855884552
====> Test set BCE loss 0.04813823848962784 Custom Loss 0.04813823848962784 with ber  0.017128001898527145 with bler  0.7846
saved model ./tmp/model_12_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.995770692825317s
====> Epoch: 13 Average loss: 0.04825574  running time 3.569981098175049
====> Epoch: 13 Average loss: 0.08457666  running time 3.576900005340576
====> Epoch: 13 Average loss: 0.08528908  running time 3.5762970447540283
====> Epoch: 13 Average loss: 0.08454974  running time 3.570666551589966
====> Epoch: 13 Average loss: 0.08419448  running time 3.5712742805480957
====> Epoch: 13 Average loss: 0.08435969  running time 3.574378490447998
====> Test set BCE loss 0.04865490272641182 Custom Loss 0.04865490272641182 with ber  0.01718199998140335 with bler  0.7831
saved model ./tmp/model_13_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.953609704971313s
====> Epoch: 14 Average loss: 0.04838542  running time 3.5660030841827393
====> Epoch: 14 Average loss: 0.08481057  running time 3.579256534576416
====> Epoch: 14 Average loss: 0.08536535  running time 3.5766255855560303
====> Epoch: 14 Average loss: 0.08532137  running time 3.5888471603393555
====> Epoch: 14 Average loss: 0.08482298  running time 3.5765750408172607
====> Epoch: 14 Average loss: 0.08495080  running time 3.604935646057129
====> Test set BCE loss 0.04805415868759155 Custom Loss 0.04805415868759155 with ber  0.017162000760436058 with bler  0.7774000000000001
saved model ./tmp/model_14_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.015065670013428s
====> Epoch: 15 Average loss: 0.04793792  running time 3.56963849067688
====> Epoch: 15 Average loss: 0.08515821  running time 3.580517530441284
====> Epoch: 15 Average loss: 0.08424740  running time 3.5902445316314697
====> Epoch: 15 Average loss: 0.08546182  running time 3.6023855209350586
====> Epoch: 15 Average loss: 0.08430841  running time 3.601940155029297
====> Epoch: 15 Average loss: 0.08408736  running time 3.5916881561279297
====> Test set BCE loss 0.04851783066987991 Custom Loss 0.04851783066987991 with ber  0.01688000187277794 with bler  0.7765
saved model ./tmp/model_15_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.072702646255493s
====> Epoch: 16 Average loss: 0.04821703  running time 3.595465898513794
====> Epoch: 16 Average loss: 0.08482011  running time 3.5998117923736572
====> Epoch: 16 Average loss: 0.08435181  running time 3.59628963470459
====> Epoch: 16 Average loss: 0.08419420  running time 3.6016526222229004
====> Epoch: 16 Average loss: 0.08445742  running time 3.595409393310547
====> Epoch: 16 Average loss: 0.08445159  running time 3.6195497512817383
====> Test set BCE loss 0.04882034286856651 Custom Loss 0.04882034286856651 with ber  0.01725899800658226 with bler  0.7823
saved model ./tmp/model_16_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.150758266448975s
====> Epoch: 17 Average loss: 0.04801355  running time 3.570828914642334
====> Epoch: 17 Average loss: 0.08495581  running time 3.5772769451141357
====> Epoch: 17 Average loss: 0.08405587  running time 3.5732178688049316
====> Epoch: 17 Average loss: 0.08579323  running time 3.578404188156128
====> Epoch: 17 Average loss: 0.08480537  running time 3.5944676399230957
====> Epoch: 17 Average loss: 0.08444800  running time 3.5804100036621094
====> Test set BCE loss 0.04880363866686821 Custom Loss 0.04880363866686821 with ber  0.017534000799059868 with bler  0.7898
saved model ./tmp/model_17_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.98599100112915s
====> Epoch: 18 Average loss: 0.04838581  running time 3.559030771255493
====> Epoch: 18 Average loss: 0.08439158  running time 3.572470188140869
====> Epoch: 18 Average loss: 0.08462496  running time 3.5811891555786133
====> Epoch: 18 Average loss: 0.08554453  running time 3.5743441581726074
====> Epoch: 18 Average loss: 0.08465290  running time 3.5690085887908936
====> Epoch: 18 Average loss: 0.08419451  running time 3.5690579414367676
====> Test set BCE loss 0.048279423266649246 Custom Loss 0.048279423266649246 with ber  0.01732500083744526 with bler  0.7871
saved model ./tmp/model_18_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.94359803199768s
====> Epoch: 19 Average loss: 0.04789769  running time 3.57143497467041
====> Epoch: 19 Average loss: 0.08526014  running time 3.5795512199401855
====> Epoch: 19 Average loss: 0.08423968  running time 3.5723249912261963
====> Epoch: 19 Average loss: 0.08468631  running time 3.579754590988159
====> Epoch: 19 Average loss: 0.08492284  running time 3.587798833847046
====> Epoch: 19 Average loss: 0.08450659  running time 3.58188796043396
====> Test set BCE loss 0.048423320055007935 Custom Loss 0.048423320055007935 with ber  0.01725500263273716 with bler  0.7852000000000001
saved model ./tmp/model_19_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.001517295837402s
====> Epoch: 20 Average loss: 0.04795695  running time 3.586782932281494
====> Epoch: 20 Average loss: 0.08547285  running time 3.587573766708374
====> Epoch: 20 Average loss: 0.08369905  running time 3.5844545364379883
====> Epoch: 20 Average loss: 0.08430818  running time 3.5883851051330566
====> Epoch: 20 Average loss: 0.08523226  running time 3.581786632537842
====> Epoch: 20 Average loss: 0.08489976  running time 3.5702686309814453
====> Test set BCE loss 0.048715557903051376 Custom Loss 0.048715557903051376 with ber  0.017030999064445496 with bler  0.7797000000000001
saved model ./tmp/model_20_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.017608404159546s
====> Epoch: 21 Average loss: 0.04901357  running time 3.567074775695801
====> Epoch: 21 Average loss: 0.08441127  running time 3.573927164077759
====> Epoch: 21 Average loss: 0.08410604  running time 3.574320077896118
====> Epoch: 21 Average loss: 0.08443443  running time 3.5792033672332764
====> Epoch: 21 Average loss: 0.08361849  running time 3.5780067443847656
====> Epoch: 21 Average loss: 0.08471699  running time 3.579285144805908
====> Test set BCE loss 0.04841567203402519 Custom Loss 0.04841567203402519 with ber  0.01702900044620037 with bler  0.783
saved model ./tmp/model_21_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.980424165725708s
====> Epoch: 22 Average loss: 0.04849291  running time 3.569511651992798
====> Epoch: 22 Average loss: 0.08410826  running time 3.5729176998138428
====> Epoch: 22 Average loss: 0.08390375  running time 3.5740013122558594
====> Epoch: 22 Average loss: 0.08374317  running time 3.571516513824463
====> Epoch: 22 Average loss: 0.08432883  running time 3.576014280319214
====> Epoch: 22 Average loss: 0.08436544  running time 3.5805485248565674
====> Test set BCE loss 0.04781533032655716 Custom Loss 0.04781533032655716 with ber  0.01717499829828739 with bler  0.7793999999999999
saved model ./tmp/model_22_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.977794885635376s
====> Epoch: 23 Average loss: 0.04794881  running time 3.572056770324707
====> Epoch: 23 Average loss: 0.08448978  running time 3.590611457824707
====> Epoch: 23 Average loss: 0.08517758  running time 3.5769498348236084
====> Epoch: 23 Average loss: 0.08444442  running time 3.5736329555511475
====> Epoch: 23 Average loss: 0.08335578  running time 3.573509931564331
====> Epoch: 23 Average loss: 0.08451207  running time 3.5819623470306396
====> Test set BCE loss 0.04817849397659302 Custom Loss 0.04817849397659302 with ber  0.0169529989361763 with bler  0.7792
saved model ./tmp/model_23_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.987026929855347s
====> Epoch: 24 Average loss: 0.04840351  running time 3.559072494506836
====> Epoch: 24 Average loss: 0.08371639  running time 3.5737411975860596
====> Epoch: 24 Average loss: 0.08474969  running time 3.582867383956909
====> Epoch: 24 Average loss: 0.08427614  running time 3.5733656883239746
====> Epoch: 24 Average loss: 0.08456510  running time 3.573885440826416
====> Epoch: 24 Average loss: 0.08496744  running time 3.58772873878479
====> Test set BCE loss 0.04884478077292442 Custom Loss 0.04884478077292442 with ber  0.017285000532865524 with bler  0.7881
saved model ./tmp/model_24_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.97487711906433s
====> Epoch: 25 Average loss: 0.04878665  running time 3.5567069053649902
====> Epoch: 25 Average loss: 0.08447804  running time 3.565847396850586
====> Epoch: 25 Average loss: 0.08497821  running time 3.566835403442383
====> Epoch: 25 Average loss: 0.08406348  running time 3.5701327323913574
====> Epoch: 25 Average loss: 0.08419399  running time 3.56483793258667
====> Epoch: 25 Average loss: 0.08413172  running time 3.5745322704315186
====> Test set BCE loss 0.04798195883631706 Custom Loss 0.04798195883631706 with ber  0.017110997810959816 with bler  0.7853
saved model ./tmp/model_25_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.920095205307007s
====> Epoch: 26 Average loss: 0.04821083  running time 3.569061517715454
====> Epoch: 26 Average loss: 0.08481159  running time 3.582289934158325
====> Epoch: 26 Average loss: 0.08476761  running time 3.57650089263916
====> Epoch: 26 Average loss: 0.08435812  running time 3.5787675380706787
====> Epoch: 26 Average loss: 0.08462135  running time 3.575310468673706
====> Epoch: 26 Average loss: 0.08410447  running time 3.578852415084839
====> Test set BCE loss 0.04853862151503563 Custom Loss 0.04853862151503563 with ber  0.01717500202357769 with bler  0.7744000000000001
saved model ./tmp/model_26_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.987533569335938s
====> Epoch: 27 Average loss: 0.04854494  running time 3.5751209259033203
====> Epoch: 27 Average loss: 0.08437357  running time 3.5857558250427246
====> Epoch: 27 Average loss: 0.08450782  running time 3.581932783126831
====> Epoch: 27 Average loss: 0.08449423  running time 3.57302188873291
====> Epoch: 27 Average loss: 0.08357441  running time 3.572505474090576
====> Epoch: 27 Average loss: 0.08350702  running time 3.5732781887054443
====> Test set BCE loss 0.04846079647541046 Custom Loss 0.04846079647541046 with ber  0.01732499897480011 with bler  0.7882
saved model ./tmp/model_27_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.985921621322632s
====> Epoch: 28 Average loss: 0.04762167  running time 3.5637054443359375
====> Epoch: 28 Average loss: 0.08305864  running time 3.585198163986206
====> Epoch: 28 Average loss: 0.08387018  running time 3.5713489055633545
====> Epoch: 28 Average loss: 0.08400380  running time 3.574470043182373
====> Epoch: 28 Average loss: 0.08493599  running time 3.5867598056793213
====> Epoch: 28 Average loss: 0.08431752  running time 3.5788931846618652
====> Test set BCE loss 0.047381944954395294 Custom Loss 0.047381944954395294 with ber  0.016857000067830086 with bler  0.7802000000000001
saved model ./tmp/model_28_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.98582172393799s
====> Epoch: 29 Average loss: 0.04807082  running time 3.5805845260620117
====> Epoch: 29 Average loss: 0.08377149  running time 3.582773208618164
====> Epoch: 29 Average loss: 0.08407829  running time 3.5700671672821045
====> Epoch: 29 Average loss: 0.08421973  running time 3.567333936691284
====> Epoch: 29 Average loss: 0.08448726  running time 3.5754988193511963
====> Epoch: 29 Average loss: 0.08486771  running time 3.569584608078003
====> Test set BCE loss 0.04880061373114586 Custom Loss 0.04880061373114586 with ber  0.017103999853134155 with bler  0.7783000000000001
saved model ./tmp/model_29_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.958784818649292s
====> Epoch: 30 Average loss: 0.04879516  running time 3.5724313259124756
====> Epoch: 30 Average loss: 0.08428952  running time 3.569143533706665
====> Epoch: 30 Average loss: 0.08381267  running time 3.56911301612854
====> Epoch: 30 Average loss: 0.08384262  running time 3.5916178226470947
====> Epoch: 30 Average loss: 0.08406237  running time 3.5766422748565674
====> Epoch: 30 Average loss: 0.08425185  running time 3.5738465785980225
====> Test set BCE loss 0.04841676354408264 Custom Loss 0.04841676354408264 with ber  0.017239002510905266 with bler  0.7812999999999999
saved model ./tmp/model_30_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.97475528717041s
====> Epoch: 31 Average loss: 0.04840137  running time 3.5563974380493164
====> Epoch: 31 Average loss: 0.08371333  running time 3.568927049636841
====> Epoch: 31 Average loss: 0.08407469  running time 3.5658485889434814
====> Epoch: 31 Average loss: 0.08378678  running time 3.575029134750366
====> Epoch: 31 Average loss: 0.08446700  running time 3.5654985904693604
====> Epoch: 31 Average loss: 0.08500949  running time 3.5694284439086914
====> Test set BCE loss 0.04774736613035202 Custom Loss 0.04774736613035202 with ber  0.016894999891519547 with bler  0.7772
saved model ./tmp/model_31_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.934088468551636s
====> Epoch: 32 Average loss: 0.04745385  running time 3.5611987113952637
====> Epoch: 32 Average loss: 0.08409183  running time 3.5781426429748535
====> Epoch: 32 Average loss: 0.08302627  running time 3.5740506649017334
====> Epoch: 32 Average loss: 0.08392537  running time 3.56801176071167
====> Epoch: 32 Average loss: 0.08459552  running time 3.5631678104400635
====> Epoch: 32 Average loss: 0.08434257  running time 3.5679163932800293
====> Test set BCE loss 0.04891060292720795 Custom Loss 0.04891060292720795 with ber  0.01750899851322174 with bler  0.7883
saved model ./tmp/model_32_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.928811073303223s
====> Epoch: 33 Average loss: 0.04839365  running time 3.5704569816589355
====> Epoch: 33 Average loss: 0.08419707  running time 3.5770504474639893
====> Epoch: 33 Average loss: 0.08496567  running time 3.579303741455078
====> Epoch: 33 Average loss: 0.08420619  running time 3.5726723670959473
====> Epoch: 33 Average loss: 0.08432973  running time 3.5706658363342285
====> Epoch: 33 Average loss: 0.08441181  running time 3.5738863945007324
====> Test set BCE loss 0.04791274294257164 Custom Loss 0.04791274294257164 with ber  0.01719999872148037 with bler  0.7798999999999999
saved model ./tmp/model_33_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.96760320663452s
====> Epoch: 34 Average loss: 0.04815206  running time 3.5752274990081787
====> Epoch: 34 Average loss: 0.08466272  running time 3.5781490802764893
====> Epoch: 34 Average loss: 0.08501859  running time 3.5865328311920166
====> Epoch: 34 Average loss: 0.08419683  running time 3.5754554271698
====> Epoch: 34 Average loss: 0.08394037  running time 3.574065685272217
====> Epoch: 34 Average loss: 0.08355549  running time 3.5673041343688965
====> Test set BCE loss 0.048097580671310425 Custom Loss 0.048097580671310425 with ber  0.017000000923871994 with bler  0.7798
saved model ./tmp/model_34_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.978402376174927s
====> Epoch: 35 Average loss: 0.04884823  running time 3.5616958141326904
====> Epoch: 35 Average loss: 0.08520460  running time 3.5797922611236572
====> Epoch: 35 Average loss: 0.08495636  running time 3.5739104747772217
====> Epoch: 35 Average loss: 0.08388957  running time 3.5708789825439453
====> Epoch: 35 Average loss: 0.08403876  running time 3.5809407234191895
====> Epoch: 35 Average loss: 0.08437048  running time 3.582526206970215
====> Test set BCE loss 0.04874986782670021 Custom Loss 0.04874986782670021 with ber  0.0172870010137558 with bler  0.7835000000000001
saved model ./tmp/model_35_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.965994596481323s
====> Epoch: 36 Average loss: 0.04831514  running time 3.5580496788024902
====> Epoch: 36 Average loss: 0.08556516  running time 3.5766050815582275
====> Epoch: 36 Average loss: 0.08433685  running time 3.5714638233184814
====> Epoch: 36 Average loss: 0.08437802  running time 3.574545383453369
====> Epoch: 36 Average loss: 0.08451920  running time 3.585956573486328
====> Epoch: 36 Average loss: 0.08479925  running time 3.5814971923828125
====> Test set BCE loss 0.04828314855694771 Custom Loss 0.04828314855694771 with ber  0.017054999247193336 with bler  0.7734
saved model ./tmp/model_36_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.978278398513794s
====> Epoch: 37 Average loss: 0.04823213  running time 3.5711092948913574
====> Epoch: 37 Average loss: 0.08528394  running time 3.5779149532318115
====> Epoch: 37 Average loss: 0.08488424  running time 3.5811405181884766
====> Epoch: 37 Average loss: 0.08450564  running time 3.5741846561431885
====> Epoch: 37 Average loss: 0.08440210  running time 3.572000026702881
====> Epoch: 37 Average loss: 0.08385795  running time 3.5743138790130615
====> Test set BCE loss 0.0479208342730999 Custom Loss 0.0479208342730999 with ber  0.01706400141119957 with bler  0.7733000000000001
saved model ./tmp/model_37_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.97652792930603s
====> Epoch: 38 Average loss: 0.04818043  running time 3.565985918045044
====> Epoch: 38 Average loss: 0.08502721  running time 3.5830743312835693
====> Epoch: 38 Average loss: 0.08387062  running time 3.5785927772521973
====> Epoch: 38 Average loss: 0.08356902  running time 3.5741539001464844
====> Epoch: 38 Average loss: 0.08480500  running time 3.5741074085235596
====> Epoch: 38 Average loss: 0.08436919  running time 3.5817086696624756
====> Test set BCE loss 0.04796626418828964 Custom Loss 0.04796626418828964 with ber  0.016933998093008995 with bler  0.7784
saved model ./tmp/model_38_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.969053268432617s
====> Epoch: 39 Average loss: 0.04808230  running time 3.5531272888183594
====> Epoch: 39 Average loss: 0.08431759  running time 3.5679843425750732
====> Epoch: 39 Average loss: 0.08353080  running time 3.5672061443328857
====> Epoch: 39 Average loss: 0.08398339  running time 3.571045398712158
====> Epoch: 39 Average loss: 0.08485426  running time 3.5690553188323975
====> Epoch: 39 Average loss: 0.08403229  running time 3.566960096359253
====> Test set BCE loss 0.04812861233949661 Custom Loss 0.04812861233949661 with ber  0.017167000100016594 with bler  0.7849
saved model ./tmp/model_39_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.907719135284424s
====> Epoch: 40 Average loss: 0.04802151  running time 3.5762925148010254
====> Epoch: 40 Average loss: 0.08376519  running time 3.576916217803955
====> Epoch: 40 Average loss: 0.08445300  running time 3.575242757797241
====> Epoch: 40 Average loss: 0.08417856  running time 3.5936901569366455
====> Epoch: 40 Average loss: 0.08452365  running time 3.5848352909088135
====> Epoch: 40 Average loss: 0.08409310  running time 3.57879638671875
====> Test set BCE loss 0.049126822501420975 Custom Loss 0.049126822501420975 with ber  0.01720999740064144 with bler  0.7877000000000001
saved model ./tmp/model_40_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.01172947883606s
====> Epoch: 41 Average loss: 0.04892115  running time 3.5783543586730957
====> Epoch: 41 Average loss: 0.08422633  running time 3.576427459716797
====> Epoch: 41 Average loss: 0.08448278  running time 3.5772271156311035
====> Epoch: 41 Average loss: 0.08425750  running time 3.5717673301696777
====> Epoch: 41 Average loss: 0.08421036  running time 3.5713584423065186
====> Epoch: 41 Average loss: 0.08414984  running time 3.5748226642608643
====> Test set BCE loss 0.04840300977230072 Custom Loss 0.04840300977230072 with ber  0.017061999067664146 with bler  0.7752000000000001
saved model ./tmp/model_41_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.968798875808716s
====> Epoch: 42 Average loss: 0.04799162  running time 3.56715726852417
====> Epoch: 42 Average loss: 0.08424696  running time 3.5730528831481934
====> Epoch: 42 Average loss: 0.08415216  running time 3.5739355087280273
====> Epoch: 42 Average loss: 0.08430867  running time 3.5701651573181152
====> Epoch: 42 Average loss: 0.08472054  running time 3.5712344646453857
====> Epoch: 42 Average loss: 0.08370739  running time 3.5711171627044678
====> Test set BCE loss 0.04875800758600235 Custom Loss 0.04875800758600235 with ber  0.017465000972151756 with bler  0.7857999999999999
saved model ./tmp/model_42_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.93961477279663s
====> Epoch: 43 Average loss: 0.04874639  running time 3.5645158290863037
====> Epoch: 43 Average loss: 0.08419913  running time 3.574903964996338
====> Epoch: 43 Average loss: 0.08462275  running time 3.5699660778045654
====> Epoch: 43 Average loss: 0.08504180  running time 3.5720455646514893
====> Epoch: 43 Average loss: 0.08456898  running time 3.573071241378784
====> Epoch: 43 Average loss: 0.08514452  running time 3.5724682807922363
====> Test set BCE loss 0.04871978238224983 Custom Loss 0.04871978238224983 with ber  0.01723799668252468 with bler  0.7834000000000001
saved model ./tmp/model_43_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.938166618347168s
====> Epoch: 44 Average loss: 0.04864175  running time 3.5777413845062256
====> Epoch: 44 Average loss: 0.08443788  running time 3.5772287845611572
====> Epoch: 44 Average loss: 0.08388471  running time 3.579328775405884
====> Epoch: 44 Average loss: 0.08476400  running time 3.5714163780212402
====> Epoch: 44 Average loss: 0.08437508  running time 3.572885036468506
====> Epoch: 44 Average loss: 0.08429428  running time 3.5740809440612793
====> Test set BCE loss 0.04885586351156235 Custom Loss 0.04885586351156235 with ber  0.017270999029278755 with bler  0.7800999999999999
saved model ./tmp/model_44_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.964860677719116s
====> Epoch: 45 Average loss: 0.04866375  running time 3.5576770305633545
====> Epoch: 45 Average loss: 0.08440383  running time 3.5733718872070312
====> Epoch: 45 Average loss: 0.08395614  running time 3.5740950107574463
====> Epoch: 45 Average loss: 0.08454646  running time 3.5804083347320557
====> Epoch: 45 Average loss: 0.08490229  running time 3.5798017978668213
====> Epoch: 45 Average loss: 0.08437900  running time 3.5715301036834717
====> Test set BCE loss 0.048368651419878006 Custom Loss 0.048368651419878006 with ber  0.01709599979221821 with bler  0.7796000000000001
saved model ./tmp/model_45_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.957157611846924s
====> Epoch: 46 Average loss: 0.04834416  running time 3.559633255004883
====> Epoch: 46 Average loss: 0.08511186  running time 3.571643352508545
====> Epoch: 46 Average loss: 0.08446339  running time 3.5739927291870117
====> Epoch: 46 Average loss: 0.08455499  running time 3.5720207691192627
====> Epoch: 46 Average loss: 0.08438736  running time 3.5684702396392822
====> Epoch: 46 Average loss: 0.08523778  running time 3.5620532035827637
====> Test set BCE loss 0.049175456166267395 Custom Loss 0.049175456166267395 with ber  0.017304999753832817 with bler  0.7851000000000001
saved model ./tmp/model_46_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.91612410545349s
====> Epoch: 47 Average loss: 0.04879161  running time 3.563781976699829
====> Epoch: 47 Average loss: 0.08481418  running time 3.5849478244781494
====> Epoch: 47 Average loss: 0.08507343  running time 3.5805835723876953
====> Epoch: 47 Average loss: 0.08556547  running time 3.5773098468780518
====> Epoch: 47 Average loss: 0.08546107  running time 3.5778868198394775
====> Epoch: 47 Average loss: 0.08529102  running time 3.5739989280700684
====> Test set BCE loss 0.04854265972971916 Custom Loss 0.04854265972971916 with ber  0.017194999381899834 with bler  0.7795000000000001
saved model ./tmp/model_47_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.97722864151001s
====> Epoch: 48 Average loss: 0.04908913  running time 3.56506609916687
====> Epoch: 48 Average loss: 0.08464230  running time 3.579319953918457
====> Epoch: 48 Average loss: 0.08419423  running time 3.5797815322875977
====> Epoch: 48 Average loss: 0.08458541  running time 3.576078414916992
====> Epoch: 48 Average loss: 0.08441002  running time 3.5814502239227295
====> Epoch: 48 Average loss: 0.08438207  running time 3.585205554962158
====> Test set BCE loss 0.04869077354669571 Custom Loss 0.04869077354669571 with ber  0.017363999038934708 with bler  0.7847000000000002
saved model ./tmp/model_48_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.982568979263306s
====> Epoch: 49 Average loss: 0.04832208  running time 3.5648703575134277
====> Epoch: 49 Average loss: 0.08439600  running time 3.5831844806671143
====> Epoch: 49 Average loss: 0.08424867  running time 3.5775182247161865
====> Epoch: 49 Average loss: 0.08425311  running time 3.586444616317749
====> Epoch: 49 Average loss: 0.08375763  running time 3.577561855316162
====> Epoch: 49 Average loss: 0.08369515  running time 3.5722711086273193
====> Test set BCE loss 0.047843750566244125 Custom Loss 0.047843750566244125 with ber  0.016989000141620636 with bler  0.7769
saved model ./tmp/model_49_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.973819732666016s
====> Epoch: 50 Average loss: 0.04863685  running time 3.5813610553741455
====> Epoch: 50 Average loss: 0.08429996  running time 3.5851259231567383
====> Epoch: 50 Average loss: 0.08405169  running time 3.5688512325286865
====> Epoch: 50 Average loss: 0.08299549  running time 3.5755178928375244
====> Epoch: 50 Average loss: 0.08437254  running time 3.5817711353302
====> Epoch: 50 Average loss: 0.08410706  running time 3.580176830291748
====> Test set BCE loss 0.04848247393965721 Custom Loss 0.04848247393965721 with ber  0.017243001610040665 with bler  0.7823999999999999
saved model ./tmp/model_50_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.98570466041565s
====> Epoch: 51 Average loss: 0.04803579  running time 3.561687469482422
====> Epoch: 51 Average loss: 0.08336066  running time 3.5719873905181885
====> Epoch: 51 Average loss: 0.08393911  running time 3.5739240646362305
====> Epoch: 51 Average loss: 0.08430893  running time 3.586127519607544
====> Epoch: 51 Average loss: 0.08362038  running time 3.5802180767059326
====> Epoch: 51 Average loss: 0.08473775  running time 3.5757198333740234
====> Test set BCE loss 0.0481974333524704 Custom Loss 0.0481974333524704 with ber  0.016892997547984123 with bler  0.7747
saved model ./tmp/model_51_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.98710560798645s
====> Epoch: 52 Average loss: 0.04801288  running time 3.5634078979492188
====> Epoch: 52 Average loss: 0.08439209  running time 3.5777108669281006
====> Epoch: 52 Average loss: 0.08422303  running time 3.57303786277771
====> Epoch: 52 Average loss: 0.08395583  running time 3.56982421875
====> Epoch: 52 Average loss: 0.08396498  running time 3.5742573738098145
====> Epoch: 52 Average loss: 0.08413729  running time 3.5781309604644775
====> Test set BCE loss 0.04887501150369644 Custom Loss 0.04887501150369644 with ber  0.017090996727347374 with bler  0.7718
saved model ./tmp/model_52_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.967347860336304s
====> Epoch: 53 Average loss: 0.04885138  running time 3.5597610473632812
====> Epoch: 53 Average loss: 0.08387370  running time 3.5700554847717285
====> Epoch: 53 Average loss: 0.08481495  running time 3.5714499950408936
====> Epoch: 53 Average loss: 0.08449551  running time 3.5651490688323975
====> Epoch: 53 Average loss: 0.08387576  running time 3.567625045776367
====> Epoch: 53 Average loss: 0.08447706  running time 3.5719821453094482
====> Test set BCE loss 0.04794606566429138 Custom Loss 0.04794606566429138 with ber  0.017176000401377678 with bler  0.7759
saved model ./tmp/model_53_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.92473578453064s
====> Epoch: 54 Average loss: 0.04733950  running time 3.5663185119628906
====> Epoch: 54 Average loss: 0.08495408  running time 3.5821471214294434
====> Epoch: 54 Average loss: 0.08430372  running time 3.587853193283081
====> Epoch: 54 Average loss: 0.08443017  running time 3.574904441833496
====> Epoch: 54 Average loss: 0.08375399  running time 3.5721569061279297
====> Epoch: 54 Average loss: 0.08520457  running time 3.5732827186584473
====> Test set BCE loss 0.04873650148510933 Custom Loss 0.04873650148510933 with ber  0.017305999994277954 with bler  0.7895
saved model ./tmp/model_54_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.97390341758728s
====> Epoch: 55 Average loss: 0.04860064  running time 3.563354969024658
====> Epoch: 55 Average loss: 0.08346805  running time 3.584505796432495
====> Epoch: 55 Average loss: 0.08433955  running time 3.5718905925750732
====> Epoch: 55 Average loss: 0.08386363  running time 3.5724730491638184
====> Epoch: 55 Average loss: 0.08345340  running time 3.5874743461608887
====> Epoch: 55 Average loss: 0.08477102  running time 3.5795280933380127
====> Test set BCE loss 0.04867275804281235 Custom Loss 0.04867275804281235 with ber  0.017392000183463097 with bler  0.7868999999999999
saved model ./tmp/model_55_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.983057737350464s
====> Epoch: 56 Average loss: 0.04762889  running time 3.5684597492218018
====> Epoch: 56 Average loss: 0.08356774  running time 3.576183557510376
====> Epoch: 56 Average loss: 0.08384423  running time 3.574648380279541
====> Epoch: 56 Average loss: 0.08381136  running time 3.5765297412872314
====> Epoch: 56 Average loss: 0.08351078  running time 3.5822501182556152
====> Epoch: 56 Average loss: 0.08456012  running time 3.5678765773773193
====> Test set BCE loss 0.04774634167551994 Custom Loss 0.04774634167551994 with ber  0.01695600152015686 with bler  0.7790000000000001
saved model ./tmp/model_56_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.955204486846924s
====> Epoch: 57 Average loss: 0.04794003  running time 3.560608386993408
====> Epoch: 57 Average loss: 0.08393389  running time 3.590162992477417
====> Epoch: 57 Average loss: 0.08485180  running time 3.5719103813171387
====> Epoch: 57 Average loss: 0.08418062  running time 3.581960439682007
====> Epoch: 57 Average loss: 0.08306720  running time 3.575669050216675
====> Epoch: 57 Average loss: 0.08371884  running time 3.5818448066711426
====> Test set BCE loss 0.04777718707919121 Custom Loss 0.04777718707919121 with ber  0.016957001760601997 with bler  0.7755000000000001
saved model ./tmp/model_57_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.97899603843689s
====> Epoch: 58 Average loss: 0.04806620  running time 3.563875913619995
====> Epoch: 58 Average loss: 0.08419634  running time 3.5797617435455322
====> Epoch: 58 Average loss: 0.08471706  running time 3.573850154876709
====> Epoch: 58 Average loss: 0.08406662  running time 3.574601888656616
====> Epoch: 58 Average loss: 0.08351513  running time 3.581423044204712
====> Epoch: 58 Average loss: 0.08430856  running time 3.5836341381073
====> Test set BCE loss 0.04798752814531326 Custom Loss 0.04798752814531326 with ber  0.01715799979865551 with bler  0.7830000000000001
saved model ./tmp/model_58_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.97822856903076s
====> Epoch: 59 Average loss: 0.04785879  running time 3.572047472000122
====> Epoch: 59 Average loss: 0.08341139  running time 3.5754175186157227
====> Epoch: 59 Average loss: 0.08364931  running time 3.577573776245117
====> Epoch: 59 Average loss: 0.08364943  running time 3.578294038772583
====> Epoch: 59 Average loss: 0.08344115  running time 3.575202226638794
====> Epoch: 59 Average loss: 0.08493033  running time 3.5730140209198
====> Test set BCE loss 0.04764997959136963 Custom Loss 0.04764997959136963 with ber  0.01685900054872036 with bler  0.7753
saved model ./tmp/model_59_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.975284814834595s
====> Epoch: 60 Average loss: 0.04814424  running time 3.5704479217529297
====> Epoch: 60 Average loss: 0.08412282  running time 3.577458620071411
====> Epoch: 60 Average loss: 0.08410694  running time 3.5703091621398926
====> Epoch: 60 Average loss: 0.08390469  running time 3.5788564682006836
====> Epoch: 60 Average loss: 0.08362473  running time 3.5729470252990723
====> Epoch: 60 Average loss: 0.08482176  running time 3.572385549545288
====> Test set BCE loss 0.0481722466647625 Custom Loss 0.0481722466647625 with ber  0.016923001036047935 with bler  0.7817000000000001
saved model ./tmp/model_60_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.968826293945312s
saved model ./tmp/model_awgn_lr_0.01_D1_10000.pt
SNRS [-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
Test SNR -1.5 with ber  0.06627800315618515 with bler 0.9965000000000002
Test SNR -1.0 with ber  0.05382900312542915 with bler 0.9894000000000001
Test SNR -0.5 with ber  0.04183200001716614 with bler 0.9724
Test SNR 0.0 with ber  0.032315995544195175 with bler 0.9352
Test SNR 0.5 with ber  0.02400200068950653 with bler 0.8787
Test SNR 1.0 with ber  0.0171509999781847 with bler 0.7811999999999999
Test SNR 1.5 with ber  0.01155600044876337 with bler 0.645
Test SNR 2.0 with ber  0.007834999822080135 with bler 0.5070999999999999
Test SNR 2.5 with ber  0.004976999945938587 with bler 0.36929999999999996
Test SNR 3.0 with ber  0.0028780000284314156 with bler 0.23959999999999998
Test SNR 3.5 with ber  0.0017359998309984803 with bler 0.15369999999999998
Test SNR 4.0 with ber  0.0008859998779371381 with bler 0.08230000000000001
final results on SNRs  [-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
BER [0.06627800315618515, 0.05382900312542915, 0.04183200001716614, 0.032315995544195175, 0.02400200068950653, 0.0171509999781847, 0.01155600044876337, 0.007834999822080135, 0.004976999945938587, 0.0028780000284314156, 0.0017359998309984803, 0.0008859998779371381]
BLER [0.9965000000000002, 0.9894000000000001, 0.9724, 0.9352, 0.8787, 0.7811999999999999, 0.645, 0.5070999999999999, 0.36929999999999996, 0.23959999999999998, 0.15369999999999998, 0.08230000000000001]
encoder power is 1.0
adjusted SNR should be [-1.4999997446509226, -1.0000000166986343, -0.49999973308696327, -0.0, 0.5000001308463472, 1.0000002900227403, 1.5000000201403676, 2.0000002404171053, 2.5000000877622415, 3.0000002493010487, 3.500000207085638, 3.999999717024358]
Training Time: 1327.6313743591309s
Namespace(D=1, batch_size=500, block_len=100, block_len_high=200, block_len_low=10, channel='awgn', code_rate_k=1, code_rate_n=3, dec_act='linear', dec_lr=0.01, dec_num_layer=5, dec_num_unit=100, dec_rnn='gru', dropout=0.0, enc_act='elu', enc_clipping='both', enc_grad_limit=0.01, enc_lr=0.01, enc_num_layer=2, enc_num_unit=25, enc_quantize_level=2, enc_rnn='gru', enc_truncate_limit=0, enc_value_limit=1.0, init_nw_weight='./tmp/model_60_awgn_lr_0.01_D1_10000_20210514-063437.pt', is_variable_block_len=False, no_code_norm=False, no_cuda=False, num_block=10000, num_epoch=120, num_train_dec=5, num_train_enc=1, optimizer='adam', precompute_norm_stats=False, print_pos_ber=False, print_pos_power=False, print_test_traj=False, radar_power=5.0, radar_prob=0.05, rec_quantize=False, snr_points=12, snr_test_end=4.0, snr_test_start=-1.5, test_ratio=1, train_channel_mode='block_norm', train_dec_channel_high=2.0, train_dec_channel_low=-1.5, train_enc_channel_high=1.0, train_enc_channel_low=1.0, vv=5)
use_cuda:  True
Channel_AE(
  (enc): ENC(
    (enc_rnn): GRU(1, 25, num_layers=2, batch_first=True)
    (enc_linear): Linear(in_features=25, out_features=3, bias=True)
  )
  (dec): DEC(
    (dropout): Dropout(p=0.0, inplace=False)
    (dec1_rnns): GRU(3, 100, num_layers=2, batch_first=True)
    (dec2_rnns): GRU(3, 100, num_layers=2, batch_first=True)
    (dec_outputs): Linear(in_features=200, out_features=1, bias=True)
  )
)
====> Epoch: 61 Average loss: 0.05694940  running time 3.5649640560150146
====> Epoch: 61 Average loss: 0.08672132  running time 3.5602939128875732
====> Epoch: 61 Average loss: 0.08491651  running time 3.5694334506988525
====> Epoch: 61 Average loss: 0.08625678  running time 3.5858652591705322
====> Epoch: 61 Average loss: 0.08541224  running time 3.566645860671997
====> Epoch: 61 Average loss: 0.08610541  running time 3.57886004447937
====> Test set BCE loss 0.04976560175418854 Custom Loss 0.04976560175418854 with ber  0.017539000138640404 with bler  0.7815999999999999
saved model ./tmp/model_61_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.957336902618408s
====> Epoch: 62 Average loss: 0.04911409  running time 3.571363687515259
====> Epoch: 62 Average loss: 0.08432533  running time 3.585777759552002
====> Epoch: 62 Average loss: 0.08478732  running time 3.5868961811065674
====> Epoch: 62 Average loss: 0.08404972  running time 3.5992705821990967
====> Epoch: 62 Average loss: 0.08441652  running time 3.598736524581909
====> Epoch: 62 Average loss: 0.08479642  running time 3.6006014347076416
====> Test set BCE loss 0.04897115007042885 Custom Loss 0.04897115007042885 with ber  0.017303001135587692 with bler  0.7825000000000001
saved model ./tmp/model_62_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.08047604560852s
====> Epoch: 63 Average loss: 0.04885462  running time 3.585404872894287
====> Epoch: 63 Average loss: 0.08424990  running time 3.5950684547424316
====> Epoch: 63 Average loss: 0.08434891  running time 3.6005451679229736
====> Epoch: 63 Average loss: 0.08385901  running time 3.5837719440460205
====> Epoch: 63 Average loss: 0.08449291  running time 3.587116241455078
====> Epoch: 63 Average loss: 0.08361887  running time 3.5913562774658203
====> Test set BCE loss 0.04858934506773949 Custom Loss 0.04858934506773949 with ber  0.017280997708439827 with bler  0.7843
saved model ./tmp/model_63_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.07329034805298s
====> Epoch: 64 Average loss: 0.04807305  running time 3.5669960975646973
====> Epoch: 64 Average loss: 0.08426554  running time 3.586270809173584
====> Epoch: 64 Average loss: 0.08398355  running time 3.580038547515869
====> Epoch: 64 Average loss: 0.08325752  running time 3.57600474357605
====> Epoch: 64 Average loss: 0.08385922  running time 3.57558536529541
====> Epoch: 64 Average loss: 0.08374416  running time 3.5696702003479004
====> Test set BCE loss 0.04854414984583855 Custom Loss 0.04854414984583855 with ber  0.01703999750316143 with bler  0.7785
saved model ./tmp/model_64_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.973474740982056s
====> Epoch: 65 Average loss: 0.04861697  running time 3.578648328781128
====> Epoch: 65 Average loss: 0.08366694  running time 3.5908005237579346
====> Epoch: 65 Average loss: 0.08439921  running time 3.6199398040771484
====> Epoch: 65 Average loss: 0.08310247  running time 3.5855369567871094
====> Epoch: 65 Average loss: 0.08368345  running time 3.5967190265655518
====> Epoch: 65 Average loss: 0.08456424  running time 3.5953848361968994
====> Test set BCE loss 0.0480579249560833 Custom Loss 0.0480579249560833 with ber  0.0169220007956028 with bler  0.7768999999999999
saved model ./tmp/model_65_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.093199729919434s
====> Epoch: 66 Average loss: 0.04763852  running time 3.5824313163757324
====> Epoch: 66 Average loss: 0.08399034  running time 3.588726758956909
====> Epoch: 66 Average loss: 0.08394078  running time 3.589322805404663
====> Epoch: 66 Average loss: 0.08401047  running time 3.5751819610595703
====> Epoch: 66 Average loss: 0.08396632  running time 3.5746638774871826
====> Epoch: 66 Average loss: 0.08408296  running time 3.5802693367004395
====> Test set BCE loss 0.0473816804587841 Custom Loss 0.0473816804587841 with ber  0.016902001574635506 with bler  0.7756000000000001
saved model ./tmp/model_66_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.00030779838562s
====> Epoch: 67 Average loss: 0.04768217  running time 3.574758768081665
====> Epoch: 67 Average loss: 0.08379334  running time 3.5843310356140137
====> Epoch: 67 Average loss: 0.08465896  running time 3.5796406269073486
====> Epoch: 67 Average loss: 0.08411795  running time 3.5809106826782227
====> Epoch: 67 Average loss: 0.08493146  running time 3.598536968231201
====> Epoch: 67 Average loss: 0.08427543  running time 3.577700614929199
====> Test set BCE loss 0.047589048743247986 Custom Loss 0.047589048743247986 with ber  0.01687599904835224 with bler  0.7786000000000001
saved model ./tmp/model_67_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.021857261657715s
====> Epoch: 68 Average loss: 0.04751490  running time 3.559666872024536
====> Epoch: 68 Average loss: 0.08430123  running time 3.574431896209717
====> Epoch: 68 Average loss: 0.08449640  running time 3.5870444774627686
====> Epoch: 68 Average loss: 0.08444388  running time 3.5699615478515625
====> Epoch: 68 Average loss: 0.08405080  running time 3.573894500732422
====> Epoch: 68 Average loss: 0.08361244  running time 3.5793039798736572
====> Test set BCE loss 0.04759616777300835 Custom Loss 0.04759616777300835 with ber  0.016923001036047935 with bler  0.7816000000000002
saved model ./tmp/model_68_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.969223737716675s
====> Epoch: 69 Average loss: 0.04764150  running time 3.5711734294891357
====> Epoch: 69 Average loss: 0.08484602  running time 3.580636978149414
====> Epoch: 69 Average loss: 0.08363653  running time 3.5968899726867676
====> Epoch: 69 Average loss: 0.08391413  running time 3.574793577194214
====> Epoch: 69 Average loss: 0.08471058  running time 3.5836715698242188
====> Epoch: 69 Average loss: 0.08409090  running time 3.5741851329803467
====> Test set BCE loss 0.048523079603910446 Custom Loss 0.048523079603910446 with ber  0.01720300130546093 with bler  0.7746000000000002
saved model ./tmp/model_69_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.99413299560547s
====> Epoch: 70 Average loss: 0.04772145  running time 3.580322027206421
====> Epoch: 70 Average loss: 0.08449363  running time 3.5699875354766846
====> Epoch: 70 Average loss: 0.08454471  running time 3.5739939212799072
====> Epoch: 70 Average loss: 0.08458118  running time 3.5710883140563965
====> Epoch: 70 Average loss: 0.08380701  running time 3.589006185531616
====> Epoch: 70 Average loss: 0.08378004  running time 3.5821056365966797
====> Test set BCE loss 0.0482381135225296 Custom Loss 0.0482381135225296 with ber  0.017014000564813614 with bler  0.7804
saved model ./tmp/model_70_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.979287147521973s
====> Epoch: 71 Average loss: 0.04799146  running time 3.565885543823242
====> Epoch: 71 Average loss: 0.08379719  running time 3.579749822616577
====> Epoch: 71 Average loss: 0.08452528  running time 3.587675094604492
====> Epoch: 71 Average loss: 0.08402981  running time 3.5793824195861816
====> Epoch: 71 Average loss: 0.08464508  running time 3.565345048904419
====> Epoch: 71 Average loss: 0.08434690  running time 3.56807017326355
====> Test set BCE loss 0.04805615171790123 Custom Loss 0.04805615171790123 with ber  0.017266998067498207 with bler  0.7802
saved model ./tmp/model_71_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.964606046676636s
====> Epoch: 72 Average loss: 0.04774950  running time 3.5847980976104736
====> Epoch: 72 Average loss: 0.08419445  running time 3.580986261367798
====> Epoch: 72 Average loss: 0.08458472  running time 3.5792958736419678
====> Epoch: 72 Average loss: 0.08479793  running time 3.596025228500366
====> Epoch: 72 Average loss: 0.08422873  running time 3.582878589630127
====> Epoch: 72 Average loss: 0.08495969  running time 3.5770106315612793
====> Test set BCE loss 0.048759978264570236 Custom Loss 0.048759978264570236 with ber  0.017288999632000923 with bler  0.7827
saved model ./tmp/model_72_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.038835525512695s
====> Epoch: 73 Average loss: 0.04838838  running time 3.565636157989502
====> Epoch: 73 Average loss: 0.08395532  running time 3.5723459720611572
====> Epoch: 73 Average loss: 0.08358889  running time 3.5713393688201904
====> Epoch: 73 Average loss: 0.08339726  running time 3.5713253021240234
====> Epoch: 73 Average loss: 0.08395433  running time 3.574784517288208
====> Epoch: 73 Average loss: 0.08346059  running time 3.5716512203216553
====> Test set BCE loss 0.04781069979071617 Custom Loss 0.04781069979071617 with ber  0.017047999426722527 with bler  0.7775000000000001
saved model ./tmp/model_73_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.930367708206177s
====> Epoch: 74 Average loss: 0.04746384  running time 3.5637941360473633
====> Epoch: 74 Average loss: 0.08353297  running time 3.5754435062408447
====> Epoch: 74 Average loss: 0.08415331  running time 3.571707248687744
====> Epoch: 74 Average loss: 0.08469664  running time 3.5816965103149414
====> Epoch: 74 Average loss: 0.08399908  running time 3.5707030296325684
====> Epoch: 74 Average loss: 0.08359138  running time 3.5763819217681885
====> Test set BCE loss 0.0479993000626564 Custom Loss 0.0479993000626564 with ber  0.016690000891685486 with bler  0.7706
saved model ./tmp/model_74_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.948650360107422s
====> Epoch: 75 Average loss: 0.04784216  running time 3.5635738372802734
====> Epoch: 75 Average loss: 0.08397779  running time 3.5761609077453613
====> Epoch: 75 Average loss: 0.08485455  running time 3.568082809448242
====> Epoch: 75 Average loss: 0.08426718  running time 3.5705296993255615
====> Epoch: 75 Average loss: 0.08453131  running time 3.5716798305511475
====> Epoch: 75 Average loss: 0.08433211  running time 3.5791120529174805
====> Test set BCE loss 0.04826463386416435 Custom Loss 0.04826463386416435 with ber  0.017109999433159828 with bler  0.78
saved model ./tmp/model_75_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.943732261657715s
====> Epoch: 76 Average loss: 0.04855607  running time 3.559202194213867
====> Epoch: 76 Average loss: 0.08456100  running time 3.57710599899292
====> Epoch: 76 Average loss: 0.08482298  running time 3.5756912231445312
====> Epoch: 76 Average loss: 0.08450695  running time 3.574925422668457
====> Epoch: 76 Average loss: 0.08358885  running time 3.5885086059570312
====> Epoch: 76 Average loss: 0.08396549  running time 3.577455997467041
====> Test set BCE loss 0.047639377415180206 Custom Loss 0.047639377415180206 with ber  0.016997000202536583 with bler  0.7742000000000001
saved model ./tmp/model_76_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.986030340194702s
====> Epoch: 77 Average loss: 0.04795927  running time 3.5574986934661865
====> Epoch: 77 Average loss: 0.08412371  running time 3.5644712448120117
====> Epoch: 77 Average loss: 0.08475260  running time 3.5687460899353027
====> Epoch: 77 Average loss: 0.08432413  running time 3.566610336303711
====> Epoch: 77 Average loss: 0.08404274  running time 3.5685300827026367
====> Epoch: 77 Average loss: 0.08418770  running time 3.580206871032715
====> Test set BCE loss 0.04729880020022392 Custom Loss 0.04729880020022392 with ber  0.01692900061607361 with bler  0.7724999999999997
saved model ./tmp/model_77_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.943849325180054s
====> Epoch: 78 Average loss: 0.04799790  running time 3.571880578994751
====> Epoch: 78 Average loss: 0.08410361  running time 3.582212448120117
====> Epoch: 78 Average loss: 0.08459869  running time 3.5781404972076416
====> Epoch: 78 Average loss: 0.08327888  running time 3.5733089447021484
====> Epoch: 78 Average loss: 0.08508755  running time 3.5686986446380615
====> Epoch: 78 Average loss: 0.08442899  running time 3.566951274871826
====> Test set BCE loss 0.049070920795202255 Custom Loss 0.049070920795202255 with ber  0.01728699915111065 with bler  0.7876000000000001
saved model ./tmp/model_78_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.956164121627808s
====> Epoch: 79 Average loss: 0.04904053  running time 3.568824529647827
====> Epoch: 79 Average loss: 0.08420671  running time 3.58573842048645
====> Epoch: 79 Average loss: 0.08501225  running time 3.5866291522979736
====> Epoch: 79 Average loss: 0.08395634  running time 3.578516721725464
====> Epoch: 79 Average loss: 0.08362486  running time 3.578655242919922
====> Epoch: 79 Average loss: 0.08459473  running time 3.5842137336730957
====> Test set BCE loss 0.04811806231737137 Custom Loss 0.04811806231737137 with ber  0.017070000991225243 with bler  0.7844
saved model ./tmp/model_79_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.002557516098022s
====> Epoch: 80 Average loss: 0.04811827  running time 3.568039655685425
====> Epoch: 80 Average loss: 0.08375264  running time 3.594414472579956
====> Epoch: 80 Average loss: 0.08442074  running time 3.5803639888763428
====> Epoch: 80 Average loss: 0.08411566  running time 3.57529354095459
====> Epoch: 80 Average loss: 0.08467030  running time 3.5773723125457764
====> Epoch: 80 Average loss: 0.08411562  running time 3.5825047492980957
====> Test set BCE loss 0.04861946031451225 Custom Loss 0.04861946031451225 with ber  0.017395997419953346 with bler  0.7892000000000001
saved model ./tmp/model_80_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.996780395507812s
====> Epoch: 81 Average loss: 0.04791832  running time 3.5740060806274414
====> Epoch: 81 Average loss: 0.08436460  running time 3.5782158374786377
====> Epoch: 81 Average loss: 0.08386364  running time 3.577228546142578
====> Epoch: 81 Average loss: 0.08403831  running time 3.5713603496551514
====> Epoch: 81 Average loss: 0.08464958  running time 3.5771594047546387
====> Epoch: 81 Average loss: 0.08438488  running time 3.5685901641845703
====> Test set BCE loss 0.04847782477736473 Custom Loss 0.04847782477736473 with ber  0.017136000096797943 with bler  0.7786000000000002
saved model ./tmp/model_81_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.9588680267334s
====> Epoch: 82 Average loss: 0.04840471  running time 3.5651705265045166
====> Epoch: 82 Average loss: 0.08414523  running time 3.5773208141326904
====> Epoch: 82 Average loss: 0.08378889  running time 3.572417736053467
====> Epoch: 82 Average loss: 0.08345978  running time 3.5766334533691406
====> Epoch: 82 Average loss: 0.08381300  running time 3.5724356174468994
====> Epoch: 82 Average loss: 0.08429986  running time 3.5715489387512207
====> Test set BCE loss 0.047500330954790115 Custom Loss 0.047500330954790115 with ber  0.01716100238263607 with bler  0.7847999999999999
saved model ./tmp/model_82_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.94935417175293s
====> Epoch: 83 Average loss: 0.04745822  running time 3.570011615753174
====> Epoch: 83 Average loss: 0.08408556  running time 3.5795340538024902
====> Epoch: 83 Average loss: 0.08524145  running time 3.581921100616455
====> Epoch: 83 Average loss: 0.08408329  running time 3.5790812969207764
====> Epoch: 83 Average loss: 0.08373964  running time 3.5844051837921143
====> Epoch: 83 Average loss: 0.08375257  running time 3.5903477668762207
====> Test set BCE loss 0.04751743748784065 Custom Loss 0.04751743748784065 with ber  0.017221003770828247 with bler  0.7814
saved model ./tmp/model_83_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.00722098350525s
====> Epoch: 84 Average loss: 0.04725273  running time 3.575291633605957
====> Epoch: 84 Average loss: 0.08442998  running time 3.582368850708008
====> Epoch: 84 Average loss: 0.08336756  running time 3.577984094619751
====> Epoch: 84 Average loss: 0.08411000  running time 3.5721912384033203
====> Epoch: 84 Average loss: 0.08431715  running time 3.5726072788238525
====> Epoch: 84 Average loss: 0.08394271  running time 3.5712838172912598
====> Test set BCE loss 0.04778200015425682 Custom Loss 0.04778200015425682 with ber  0.016908003017306328 with bler  0.7758999999999999
saved model ./tmp/model_84_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.95434594154358s
====> Epoch: 85 Average loss: 0.04784132  running time 3.5662810802459717
====> Epoch: 85 Average loss: 0.08336927  running time 3.5796260833740234
====> Epoch: 85 Average loss: 0.08418095  running time 3.5758132934570312
====> Epoch: 85 Average loss: 0.08343842  running time 3.5737245082855225
====> Epoch: 85 Average loss: 0.08416290  running time 3.5810060501098633
====> Epoch: 85 Average loss: 0.08430206  running time 3.577563524246216
====> Test set BCE loss 0.04858119413256645 Custom Loss 0.04858119413256645 with ber  0.01726899854838848 with bler  0.7826000000000001
saved model ./tmp/model_85_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.974690198898315s
====> Epoch: 86 Average loss: 0.04846902  running time 3.57019305229187
====> Epoch: 86 Average loss: 0.08443521  running time 3.580782413482666
====> Epoch: 86 Average loss: 0.08373943  running time 3.5785155296325684
====> Epoch: 86 Average loss: 0.08361377  running time 3.5782113075256348
====> Epoch: 86 Average loss: 0.08394701  running time 3.574364423751831
====> Epoch: 86 Average loss: 0.08413175  running time 3.580502986907959
====> Test set BCE loss 0.04748542606830597 Custom Loss 0.04748542606830597 with ber  0.01706100068986416 with bler  0.7855
saved model ./tmp/model_86_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.983126401901245s
====> Epoch: 87 Average loss: 0.04761176  running time 3.5637145042419434
====> Epoch: 87 Average loss: 0.08392859  running time 3.578615427017212
====> Epoch: 87 Average loss: 0.08422605  running time 3.57946515083313
====> Epoch: 87 Average loss: 0.08397079  running time 3.5789756774902344
====> Epoch: 87 Average loss: 0.08436426  running time 3.588228702545166
====> Epoch: 87 Average loss: 0.08374712  running time 3.577963352203369
====> Test set BCE loss 0.04842716455459595 Custom Loss 0.04842716455459595 with ber  0.01697400026023388 with bler  0.7802000000000001
saved model ./tmp/model_87_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.98161792755127s
====> Epoch: 88 Average loss: 0.04825402  running time 3.570162296295166
====> Epoch: 88 Average loss: 0.08427463  running time 3.578010082244873
====> Epoch: 88 Average loss: 0.08374793  running time 3.5755386352539062
====> Epoch: 88 Average loss: 0.08369944  running time 3.5789341926574707
====> Epoch: 88 Average loss: 0.08396476  running time 3.5729711055755615
====> Epoch: 88 Average loss: 0.08394734  running time 3.5779292583465576
====> Test set BCE loss 0.04799087345600128 Custom Loss 0.04799087345600128 with ber  0.017094001173973083 with bler  0.7790000000000001
saved model ./tmp/model_88_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.96956777572632s
====> Epoch: 89 Average loss: 0.04777752  running time 3.572639226913452
====> Epoch: 89 Average loss: 0.08368493  running time 3.583754301071167
====> Epoch: 89 Average loss: 0.08411386  running time 3.595221519470215
====> Epoch: 89 Average loss: 0.08449216  running time 3.5757222175598145
====> Epoch: 89 Average loss: 0.08430350  running time 3.579078197479248
====> Epoch: 89 Average loss: 0.08434992  running time 3.572766065597534
====> Test set BCE loss 0.04794701561331749 Custom Loss 0.04794701561331749 with ber  0.017122002318501472 with bler  0.7768000000000002
saved model ./tmp/model_89_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.018737077713013s
====> Epoch: 90 Average loss: 0.04727506  running time 3.5696980953216553
====> Epoch: 90 Average loss: 0.08320060  running time 3.5783257484436035
====> Epoch: 90 Average loss: 0.08488809  running time 3.5859971046447754
====> Epoch: 90 Average loss: 0.08456383  running time 3.5998237133026123
====> Epoch: 90 Average loss: 0.08355697  running time 3.6081807613372803
====> Epoch: 90 Average loss: 0.08449617  running time 3.594719648361206
====> Test set BCE loss 0.04845995828509331 Custom Loss 0.04845995828509331 with ber  0.01726900041103363 with bler  0.7788
saved model ./tmp/model_90_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.078079223632812s
====> Epoch: 91 Average loss: 0.04811770  running time 3.5718038082122803
====> Epoch: 91 Average loss: 0.08391170  running time 3.583200454711914
====> Epoch: 91 Average loss: 0.08498630  running time 3.578359365463257
====> Epoch: 91 Average loss: 0.08484414  running time 3.586087465286255
====> Epoch: 91 Average loss: 0.08426825  running time 3.579263210296631
====> Epoch: 91 Average loss: 0.08386336  running time 3.577054262161255
====> Test set BCE loss 0.04754224792122841 Custom Loss 0.04754224792122841 with ber  0.01671300083398819 with bler  0.7702
saved model ./tmp/model_91_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.995306730270386s
====> Epoch: 92 Average loss: 0.04789919  running time 3.5737311840057373
====> Epoch: 92 Average loss: 0.08383322  running time 3.5833840370178223
====> Epoch: 92 Average loss: 0.08300350  running time 3.5738167762756348
====> Epoch: 92 Average loss: 0.08460034  running time 3.5716826915740967
====> Epoch: 92 Average loss: 0.08367678  running time 3.571152448654175
====> Epoch: 92 Average loss: 0.08412499  running time 3.577396869659424
====> Test set BCE loss 0.048247963190078735 Custom Loss 0.048247963190078735 with ber  0.017041997984051704 with bler  0.775
saved model ./tmp/model_92_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.98164701461792s
====> Epoch: 93 Average loss: 0.04839898  running time 3.5761051177978516
====> Epoch: 93 Average loss: 0.08467407  running time 3.5933103561401367
====> Epoch: 93 Average loss: 0.08396833  running time 3.5869176387786865
====> Epoch: 93 Average loss: 0.08401058  running time 3.579094886779785
====> Epoch: 93 Average loss: 0.08431990  running time 3.582719564437866
====> Epoch: 93 Average loss: 0.08424289  running time 3.588229179382324
====> Test set BCE loss 0.0480511449277401 Custom Loss 0.0480511449277401 with ber  0.01711299829185009 with bler  0.7845000000000001
saved model ./tmp/model_93_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.033084869384766s
====> Epoch: 94 Average loss: 0.04781524  running time 3.5701963901519775
====> Epoch: 94 Average loss: 0.08408916  running time 3.588998556137085
====> Epoch: 94 Average loss: 0.08450558  running time 3.596741199493408
====> Epoch: 94 Average loss: 0.08400509  running time 3.5864741802215576
====> Epoch: 94 Average loss: 0.08373057  running time 3.599968194961548
====> Epoch: 94 Average loss: 0.08322629  running time 3.582550287246704
====> Test set BCE loss 0.04714878648519516 Custom Loss 0.04714878648519516 with ber  0.016808999702334404 with bler  0.7739
saved model ./tmp/model_94_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.061322450637817s
====> Epoch: 95 Average loss: 0.04766728  running time 3.5695180892944336
====> Epoch: 95 Average loss: 0.08392303  running time 3.5747668743133545
====> Epoch: 95 Average loss: 0.08277095  running time 3.572784423828125
====> Epoch: 95 Average loss: 0.08297418  running time 3.5747408866882324
====> Epoch: 95 Average loss: 0.08430973  running time 3.575704574584961
====> Epoch: 95 Average loss: 0.08444334  running time 3.593639612197876
====> Test set BCE loss 0.04759710282087326 Custom Loss 0.04759710282087326 with ber  0.01690000109374523 with bler  0.7748000000000002
saved model ./tmp/model_95_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.98269295692444s
====> Epoch: 96 Average loss: 0.04803155  running time 3.5794525146484375
====> Epoch: 96 Average loss: 0.08356009  running time 3.5802202224731445
====> Epoch: 96 Average loss: 0.08391458  running time 3.577667236328125
====> Epoch: 96 Average loss: 0.08418161  running time 3.5817408561706543
====> Epoch: 96 Average loss: 0.08419631  running time 3.575880289077759
====> Epoch: 96 Average loss: 0.08465068  running time 3.5767858028411865
====> Test set BCE loss 0.048761021345853806 Custom Loss 0.048761021345853806 with ber  0.01734199933707714 with bler  0.7824
saved model ./tmp/model_96_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.992968559265137s
====> Epoch: 97 Average loss: 0.04835364  running time 3.5742831230163574
====> Epoch: 97 Average loss: 0.08413528  running time 3.5792131423950195
====> Epoch: 97 Average loss: 0.08400909  running time 3.5916709899902344
====> Epoch: 97 Average loss: 0.08378750  running time 3.5902230739593506
====> Epoch: 97 Average loss: 0.08387622  running time 3.5881731510162354
====> Epoch: 97 Average loss: 0.08300168  running time 3.58799409866333
====> Test set BCE loss 0.04753253981471062 Custom Loss 0.04753253981471062 with ber  0.01704300008714199 with bler  0.7770000000000001
saved model ./tmp/model_97_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.039807081222534s
====> Epoch: 98 Average loss: 0.04807587  running time 3.574676752090454
====> Epoch: 98 Average loss: 0.08435834  running time 3.583652973175049
====> Epoch: 98 Average loss: 0.08375308  running time 3.5874745845794678
====> Epoch: 98 Average loss: 0.08389287  running time 3.582742214202881
====> Epoch: 98 Average loss: 0.08368736  running time 3.5898027420043945
====> Epoch: 98 Average loss: 0.08373995  running time 3.584442377090454
====> Test set BCE loss 0.04757026955485344 Custom Loss 0.04757026955485344 with ber  0.016746999695897102 with bler  0.7797
saved model ./tmp/model_98_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.030481338500977s
====> Epoch: 99 Average loss: 0.04796349  running time 3.56947660446167
====> Epoch: 99 Average loss: 0.08456612  running time 3.584864377975464
====> Epoch: 99 Average loss: 0.08371766  running time 3.578033208847046
====> Epoch: 99 Average loss: 0.08320192  running time 3.575432300567627
====> Epoch: 99 Average loss: 0.08393383  running time 3.5736451148986816
====> Epoch: 99 Average loss: 0.08425393  running time 3.573509454727173
====> Test set BCE loss 0.04821566864848137 Custom Loss 0.04821566864848137 with ber  0.017163999378681183 with bler  0.7774
saved model ./tmp/model_99_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 21.98136067390442s
====> Epoch: 100 Average loss: 0.04780622  running time 3.575561046600342
====> Epoch: 100 Average loss: 0.08377045  running time 3.593595504760742
====> Epoch: 100 Average loss: 0.08356629  running time 3.5883755683898926
====> Epoch: 100 Average loss: 0.08367267  running time 3.5881214141845703
====> Epoch: 100 Average loss: 0.08362353  running time 3.585859537124634
====> Epoch: 100 Average loss: 0.08349134  running time 3.5923328399658203
====> Test set BCE loss 0.04735758528113365 Custom Loss 0.04735758528113365 with ber  0.016860999166965485 with bler  0.7777
saved model ./tmp/model_100_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.048800230026245s
====> Epoch: 101 Average loss: 0.04794633  running time 3.5804107189178467
====> Epoch: 101 Average loss: 0.08347671  running time 3.5865118503570557
====> Epoch: 101 Average loss: 0.08363783  running time 3.591416835784912
====> Epoch: 101 Average loss: 0.08415735  running time 3.591688871383667
====> Epoch: 101 Average loss: 0.08396766  running time 3.603365659713745
====> Epoch: 101 Average loss: 0.08295213  running time 3.596315383911133
====> Test set BCE loss 0.047163572162389755 Custom Loss 0.047163572162389755 with ber  0.01685499958693981 with bler  0.7759999999999999
saved model ./tmp/model_101_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.06810188293457s
====> Epoch: 102 Average loss: 0.04723691  running time 3.587233304977417
====> Epoch: 102 Average loss: 0.08397915  running time 3.584137439727783
====> Epoch: 102 Average loss: 0.08393509  running time 3.5826547145843506
====> Epoch: 102 Average loss: 0.08422151  running time 3.5895376205444336
====> Epoch: 102 Average loss: 0.08374975  running time 3.6016764640808105
====> Epoch: 102 Average loss: 0.08334834  running time 3.585116147994995
====> Test set BCE loss 0.04746707156300545 Custom Loss 0.04746707156300545 with ber  0.017069002613425255 with bler  0.7757
saved model ./tmp/model_102_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.08606266975403s
====> Epoch: 103 Average loss: 0.04837767  running time 3.575829029083252
====> Epoch: 103 Average loss: 0.08357581  running time 3.5904645919799805
====> Epoch: 103 Average loss: 0.08439679  running time 3.583507776260376
====> Epoch: 103 Average loss: 0.08401878  running time 3.5810530185699463
====> Epoch: 103 Average loss: 0.08341063  running time 3.5850560665130615
====> Epoch: 103 Average loss: 0.08310236  running time 3.582503318786621
====> Test set BCE loss 0.04783860966563225 Custom Loss 0.04783860966563225 with ber  0.017138998955488205 with bler  0.78
saved model ./tmp/model_103_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.026634454727173s
====> Epoch: 104 Average loss: 0.04732730  running time 3.58050799369812
====> Epoch: 104 Average loss: 0.08479066  running time 3.5864675045013428
====> Epoch: 104 Average loss: 0.08449163  running time 3.587144374847412
====> Epoch: 104 Average loss: 0.08424272  running time 3.592463731765747
====> Epoch: 104 Average loss: 0.08402238  running time 3.5908939838409424
====> Epoch: 104 Average loss: 0.08369901  running time 3.606003522872925
====> Test set BCE loss 0.04801670089364052 Custom Loss 0.04801670089364052 with ber  0.0170810017734766 with bler  0.7792
saved model ./tmp/model_104_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.086045026779175s
====> Epoch: 105 Average loss: 0.04801970  running time 3.587308645248413
====> Epoch: 105 Average loss: 0.08351902  running time 3.5909500122070312
====> Epoch: 105 Average loss: 0.08405780  running time 3.583096504211426
====> Epoch: 105 Average loss: 0.08360584  running time 3.581909418106079
====> Epoch: 105 Average loss: 0.08294400  running time 3.585982322692871
====> Epoch: 105 Average loss: 0.08514513  running time 3.581106185913086
====> Test set BCE loss 0.04812204837799072 Custom Loss 0.04812204837799072 with ber  0.017018998041749 with bler  0.7776
saved model ./tmp/model_105_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.034331798553467s
====> Epoch: 106 Average loss: 0.04837769  running time 3.5723679065704346
====> Epoch: 106 Average loss: 0.08396249  running time 3.5788376331329346
====> Epoch: 106 Average loss: 0.08524239  running time 3.5866785049438477
====> Epoch: 106 Average loss: 0.08422223  running time 3.5817129611968994
====> Epoch: 106 Average loss: 0.08387184  running time 3.6082265377044678
====> Epoch: 106 Average loss: 0.08436293  running time 3.5800728797912598
====> Test set BCE loss 0.0483456514775753 Custom Loss 0.0483456514775753 with ber  0.017207002267241478 with bler  0.7759999999999999
saved model ./tmp/model_106_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.042446851730347s
====> Epoch: 107 Average loss: 0.04769171  running time 3.579348087310791
====> Epoch: 107 Average loss: 0.08369533  running time 3.596109390258789
====> Epoch: 107 Average loss: 0.08436526  running time 3.5898075103759766
====> Epoch: 107 Average loss: 0.08519362  running time 3.5821831226348877
====> Epoch: 107 Average loss: 0.08454065  running time 3.586142063140869
====> Epoch: 107 Average loss: 0.08497353  running time 3.593580722808838
====> Test set BCE loss 0.04827874153852463 Custom Loss 0.04827874153852463 with ber  0.017066000029444695 with bler  0.7737
saved model ./tmp/model_107_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.051455974578857s
====> Epoch: 108 Average loss: 0.04835587  running time 3.5842437744140625
====> Epoch: 108 Average loss: 0.08437983  running time 3.5850725173950195
====> Epoch: 108 Average loss: 0.08437556  running time 3.582836151123047
====> Epoch: 108 Average loss: 0.08394059  running time 3.5810256004333496
====> Epoch: 108 Average loss: 0.08383304  running time 3.595644950866699
====> Epoch: 108 Average loss: 0.08372673  running time 3.585747003555298
====> Test set BCE loss 0.04781448096036911 Custom Loss 0.04781448096036911 with ber  0.0171549990773201 with bler  0.7874999999999999
saved model ./tmp/model_108_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.03939723968506s
====> Epoch: 109 Average loss: 0.04817272  running time 3.5764689445495605
====> Epoch: 109 Average loss: 0.08428682  running time 3.5830092430114746
====> Epoch: 109 Average loss: 0.08369315  running time 3.610834836959839
====> Epoch: 109 Average loss: 0.08438878  running time 3.5965569019317627
====> Epoch: 109 Average loss: 0.08330852  running time 3.591062068939209
====> Epoch: 109 Average loss: 0.08488074  running time 3.5843422412872314
====> Test set BCE loss 0.04817018285393715 Custom Loss 0.04817018285393715 with ber  0.01721399836242199 with bler  0.7832000000000001
saved model ./tmp/model_109_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.06840705871582s
====> Epoch: 110 Average loss: 0.04820312  running time 3.5784287452697754
====> Epoch: 110 Average loss: 0.08365097  running time 3.5829360485076904
====> Epoch: 110 Average loss: 0.08336555  running time 3.5844926834106445
====> Epoch: 110 Average loss: 0.08388729  running time 3.583388090133667
====> Epoch: 110 Average loss: 0.08395223  running time 3.5842716693878174
====> Epoch: 110 Average loss: 0.08423214  running time 3.5854105949401855
====> Test set BCE loss 0.04806696996092796 Custom Loss 0.04806696996092796 with ber  0.01698100008070469 with bler  0.776
saved model ./tmp/model_110_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.027273416519165s
====> Epoch: 111 Average loss: 0.04862684  running time 3.5814969539642334
====> Epoch: 111 Average loss: 0.08344267  running time 3.5915210247039795
====> Epoch: 111 Average loss: 0.08418845  running time 3.5816986560821533
====> Epoch: 111 Average loss: 0.08352109  running time 3.592924118041992
====> Epoch: 111 Average loss: 0.08359640  running time 3.599153518676758
====> Epoch: 111 Average loss: 0.08404835  running time 3.58660626411438
====> Test set BCE loss 0.04791133850812912 Custom Loss 0.04791133850812912 with ber  0.017219997942447662 with bler  0.7822000000000001
saved model ./tmp/model_111_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.05698323249817s
====> Epoch: 112 Average loss: 0.04780016  running time 3.5769155025482178
====> Epoch: 112 Average loss: 0.08358344  running time 3.6021618843078613
====> Epoch: 112 Average loss: 0.08326599  running time 3.588655471801758
====> Epoch: 112 Average loss: 0.08392166  running time 3.584583044052124
====> Epoch: 112 Average loss: 0.08509926  running time 3.5809504985809326
====> Epoch: 112 Average loss: 0.08353988  running time 3.595691680908203
====> Test set BCE loss 0.04774152487516403 Custom Loss 0.04774152487516403 with ber  0.01709500141441822 with bler  0.7839
saved model ./tmp/model_112_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.05302143096924s
====> Epoch: 113 Average loss: 0.04719063  running time 3.574383020401001
====> Epoch: 113 Average loss: 0.08446392  running time 3.5822134017944336
====> Epoch: 113 Average loss: 0.08335840  running time 3.5827503204345703
====> Epoch: 113 Average loss: 0.08385702  running time 3.584505558013916
====> Epoch: 113 Average loss: 0.08422705  running time 3.5816383361816406
====> Epoch: 113 Average loss: 0.08369641  running time 3.5811398029327393
====> Test set BCE loss 0.04814651608467102 Custom Loss 0.04814651608467102 with ber  0.016923999413847923 with bler  0.7788999999999999
saved model ./tmp/model_113_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.01231288909912s
====> Epoch: 114 Average loss: 0.04805663  running time 3.5737175941467285
====> Epoch: 114 Average loss: 0.08427244  running time 3.5961496829986572
====> Epoch: 114 Average loss: 0.08417033  running time 3.578395366668701
====> Epoch: 114 Average loss: 0.08370169  running time 3.586491107940674
====> Epoch: 114 Average loss: 0.08429550  running time 3.5833067893981934
====> Epoch: 114 Average loss: 0.08366066  running time 3.597515821456909
====> Test set BCE loss 0.04772350937128067 Custom Loss 0.04772350937128067 with ber  0.01692800037562847 with bler  0.7722
saved model ./tmp/model_114_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.048775911331177s
====> Epoch: 115 Average loss: 0.04770304  running time 3.577285051345825
====> Epoch: 115 Average loss: 0.08342488  running time 3.5844826698303223
====> Epoch: 115 Average loss: 0.08371782  running time 3.5961828231811523
====> Epoch: 115 Average loss: 0.08381631  running time 3.583312749862671
====> Epoch: 115 Average loss: 0.08400173  running time 3.5836288928985596
====> Epoch: 115 Average loss: 0.08365760  running time 3.591243267059326
====> Test set BCE loss 0.047944266349077225 Custom Loss 0.047944266349077225 with ber  0.01724500022828579 with bler  0.7824000000000002
saved model ./tmp/model_115_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.035648822784424s
====> Epoch: 116 Average loss: 0.04792998  running time 3.5691277980804443
====> Epoch: 116 Average loss: 0.08398883  running time 3.584564447402954
====> Epoch: 116 Average loss: 0.08396570  running time 3.5951461791992188
====> Epoch: 116 Average loss: 0.08418166  running time 3.59053897857666
====> Epoch: 116 Average loss: 0.08471246  running time 3.5882420539855957
====> Epoch: 116 Average loss: 0.08383298  running time 3.5928592681884766
====> Test set BCE loss 0.0479138121008873 Custom Loss 0.0479138121008873 with ber  0.01686999946832657 with bler  0.7770999999999999
saved model ./tmp/model_116_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.059946537017822s
====> Epoch: 117 Average loss: 0.04831758  running time 3.577659845352173
====> Epoch: 117 Average loss: 0.08341039  running time 3.579406976699829
====> Epoch: 117 Average loss: 0.08450855  running time 3.5751864910125732
====> Epoch: 117 Average loss: 0.08417377  running time 3.5795934200286865
====> Epoch: 117 Average loss: 0.08404391  running time 3.5851058959960938
====> Epoch: 117 Average loss: 0.08422900  running time 3.5867574214935303
====> Test set BCE loss 0.04808173328638077 Custom Loss 0.04808173328638077 with ber  0.01721700094640255 with bler  0.7780000000000001
saved model ./tmp/model_117_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.018604040145874s
====> Epoch: 118 Average loss: 0.04806589  running time 3.5769011974334717
====> Epoch: 118 Average loss: 0.08335359  running time 3.5868496894836426
====> Epoch: 118 Average loss: 0.08354207  running time 3.60086989402771
====> Epoch: 118 Average loss: 0.08423631  running time 3.6066062450408936
====> Epoch: 118 Average loss: 0.08393514  running time 3.5921974182128906
====> Epoch: 118 Average loss: 0.08397073  running time 3.6048943996429443
====> Test set BCE loss 0.048648178577423096 Custom Loss 0.048648178577423096 with ber  0.01723499968647957 with bler  0.7859999999999998
saved model ./tmp/model_118_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.09616231918335s
====> Epoch: 119 Average loss: 0.04819846  running time 3.5837326049804688
====> Epoch: 119 Average loss: 0.08402904  running time 3.5851807594299316
====> Epoch: 119 Average loss: 0.08429513  running time 3.5918917655944824
====> Epoch: 119 Average loss: 0.08425904  running time 3.5821127891540527
====> Epoch: 119 Average loss: 0.08386759  running time 3.579576253890991
====> Epoch: 119 Average loss: 0.08311557  running time 3.585131883621216
====> Test set BCE loss 0.04762433469295502 Custom Loss 0.04762433469295502 with ber  0.01709200069308281 with bler  0.7774000000000001
saved model ./tmp/model_119_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.035913467407227s
====> Epoch: 120 Average loss: 0.04744486  running time 3.567047595977783
====> Epoch: 120 Average loss: 0.08335274  running time 3.573685884475708
====> Epoch: 120 Average loss: 0.08385671  running time 3.612851858139038
====> Epoch: 120 Average loss: 0.08394996  running time 3.582507610321045
====> Epoch: 120 Average loss: 0.08396306  running time 3.5848042964935303
====> Epoch: 120 Average loss: 0.08415052  running time 3.5771028995513916
====> Test set BCE loss 0.04694614186882973 Custom Loss 0.04694614186882973 with ber  0.016674000769853592 with bler  0.7724
saved model ./tmp/model_120_awgn_lr_0.01_D1_10000_20210514-063437.pt
each epoch training time: 22.014036893844604s
saved model ./tmp/model_awgn_lr_0.01_D1_10000.pt
SNRS [-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
Test SNR -1.5 with ber  0.06662499159574509 with bler 0.9964999999999999
Test SNR -1.0 with ber  0.054013997316360474 with bler 0.9902999999999998
Test SNR -0.5 with ber  0.04218999668955803 with bler 0.9718
Test SNR 0.0 with ber  0.03216199576854706 with bler 0.9379
Test SNR 0.5 with ber  0.023822998628020287 with bler 0.8751
Test SNR 1.0 with ber  0.01733500137925148 with bler 0.7842
Test SNR 1.5 with ber  0.01143999956548214 with bler 0.6458999999999999
Test SNR 2.0 with ber  0.007624998688697815 with bler 0.5028
Test SNR 2.5 with ber  0.004821000620722771 with bler 0.35530000000000006
Test SNR 3.0 with ber  0.0028699999675154686 with bler 0.23699999999999996
Test SNR 3.5 with ber  0.0016509998822584748 with bler 0.1437
Test SNR 4.0 with ber  0.0008640001760795712 with bler 0.07880000000000002
final results on SNRs  [-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
BER [0.06662499159574509, 0.054013997316360474, 0.04218999668955803, 0.03216199576854706, 0.023822998628020287, 0.01733500137925148, 0.01143999956548214, 0.007624998688697815, 0.004821000620722771, 0.0028699999675154686, 0.0016509998822584748, 0.0008640001760795712]
BLER [0.9964999999999999, 0.9902999999999998, 0.9718, 0.9379, 0.8751, 0.7842, 0.6458999999999999, 0.5028, 0.35530000000000006, 0.23699999999999996, 0.1437, 0.07880000000000002]
encoder power is 1.0
adjusted SNR should be [-1.4999997446509226, -1.0000000166986343, -0.49999973308696327, -0.0, 0.5000001308463472, 1.0000002900227403, 1.5000000201403676, 2.0000002404171053, 2.5000000877622415, 3.0000002493010487, 3.500000207085638, 3.999999717024358]
Training Time: 1330.4319956302643s
